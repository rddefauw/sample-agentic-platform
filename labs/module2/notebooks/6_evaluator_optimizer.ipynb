{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Answer Improver: The Evaluator-Optimizer Pattern\n",
    "\n",
    "Welcome to our final notebook! The evaluator optimizer technique works by having another prompt \"interrogate\" the answer previously given. That feedback is then used to improve the result. The evaluator-optimizer pattern shines when:\n",
    "- We have clear evaluation criteria\n",
    "- Iterative refinement adds measurable value\n",
    "- LLM feedback can meaningfully improve responses\n",
    "\n",
    "Perfect for improving OpenSearch answers! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "from utils.retrieval_utils import get_chroma_os_docs_collection, ChromaDBRetrievalClient\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "session = boto3.Session()\n",
    "bedrock = session.client(service_name='bedrock-runtime')\n",
    "\n",
    "# We've pushed the retrieval client from the prompt chaining notebook to the retrieval utils for simplicity\n",
    "chroma_os_docs_collection: ChromaDBRetrievalClient = get_chroma_os_docs_collection()\n",
    "\n",
    "print(\"‚úÖ Client setup and retrieval client complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Helpers\n",
    "Reuse the same helpers from our previous workshops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, Dict, Any, List\n",
    "\n",
    "# We pushed the base propmt from the previous lab to a a base prompt file.\n",
    "from utils.base_prompt import BasePrompt\n",
    "from utils.retrieval_utils import RetrievalResult\n",
    "\n",
    "def call_bedrock(prompt: BasePrompt) -> str:\n",
    "    kwargs = {\n",
    "        \"modelId\": prompt.model_id,\n",
    "        \"inferenceConfig\": prompt.hyperparams,\n",
    "        \"messages\": prompt.to_bedrock_messages(),\n",
    "        \"system\": prompt.to_bedrock_system(),\n",
    "    }\n",
    "\n",
    "    # Call Bedrock\n",
    "    converse_response: Dict[str, Any] = bedrock.converse(**kwargs)\n",
    "    # Get the model's text response\n",
    "    return converse_response['output']['message']['content'][0]['text']\n",
    "\n",
    "# Helper function to call bedrock\n",
    "# def do_rag(user_input: str, rag_prompt: Type[BasePrompt]) -> str:\n",
    "#     # Retrieve the context from the vector store\n",
    "#     retrieval_results: List[RetrievalResult] = chroma_os_docs_collection.retrieve(user_input, n_results=2)\n",
    "#     # Format the context into a string\n",
    "#     context: str = \"\\n\\n\".join([result.document for result in retrieval_results])\n",
    "\n",
    "#     print(\"Retrieval done\")\n",
    "#     # Create the RAG prompt\n",
    "#     inputs: Dict[str, Any] = {\"question\": user_input, \"context\": context}\n",
    "#     rag_prompt: BasePrompt = rag_prompt(inputs=inputs)\n",
    "#     # Call Bedrock with the RAG prompt\n",
    "\n",
    "#     print(\"Calling Bedrock\")\n",
    "#     return call_bedrock(rag_prompt)\n",
    "\n",
    "def do_retrieve(query: str) -> str:\n",
    "    \"\"\"Retrieves the context from the vector store\"\"\"\n",
    "    retrieval_results: List[RetrievalResult] = chroma_os_docs_collection.retrieve(query, n_results=2)\n",
    "    # Format the context into a string\n",
    "    return  \"\\n\\n\".join([result.document for result in retrieval_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Our Answer Improvement System\n",
    "\n",
    "We'll build a system that:\n",
    "1. Generates initial answers\n",
    "2. Evaluates them for quality\n",
    "3. Provides specific improvement feedback\n",
    "4. Iteratively refines the answer\n",
    "\n",
    "First lets create our prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.base_prompt import BasePrompt\n",
    "\n",
    "# Define system prompt\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert OpenSearch troubleshooter who provides accurate, comprehensive solutions.\n",
    "\"\"\"\n",
    "\n",
    "DECISION_SYSTEM_PROMPT = \"You make clear decisions based on feedback quality.\"\n",
    "\n",
    "# Define prompt templates as constants\n",
    "GENERATE_SOLUTION_TEMPLATE = \"\"\"\n",
    "Provide a comprehensive troubleshooting solution for this OpenSearch issue:\n",
    "\n",
    "<query>\n",
    "{question}\n",
    "</query>\n",
    "\n",
    "<documentation>\n",
    "{context}\n",
    "</documentation>\n",
    "\n",
    "Include:\n",
    "- Potential root causes\n",
    "- Diagnostic steps\n",
    "- Resolution instructions\n",
    "- Verification methods\n",
    "\"\"\"\n",
    "\n",
    "EVALUATE_SOLUTION_TEMPLATE = \"\"\"\n",
    "Evaluate if this OpenSearch troubleshooting solution fully addresses the problem:\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\n",
    "Assess:\n",
    "- Completeness\n",
    "- Technical accuracy\n",
    "- Practical applicability\n",
    "- Clarity of explanation\n",
    "\n",
    "Provide specific feedback for improvement.\n",
    "\"\"\"\n",
    "\n",
    "IMPROVE_SOLUTION_TEMPLATE = \"\"\"\n",
    "Improve this OpenSearch troubleshooting solution based on feedback:\n",
    "\n",
    "<problem>\n",
    "{question}\n",
    "</problem>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<current_solution>\n",
    "{answer}\n",
    "</current_solution>\n",
    "\n",
    "<feedback>\n",
    "{feedback}\n",
    "</feedback>\n",
    "\n",
    "Provide an improved solution that addresses all feedback points.\n",
    "\"\"\"\n",
    "\n",
    "DECISION_PROMPT_TEMPLATE = \"\"\"\n",
    "Based on this feedback, does the OpenSearch troubleshooting solution need improvement?\n",
    "\n",
    "<feedback>\n",
    "{feedback}\n",
    "</feedback>\n",
    "\n",
    "Reply with ONLY with 'IMPROVE' or 'DONE'.\n",
    "\"\"\"\n",
    "\n",
    "# Define prompt classes\n",
    "class GenerateSolutionPrompt(BasePrompt):\n",
    "    system_prompt: str = SYSTEM_PROMPT\n",
    "    user_prompt: str = GENERATE_SOLUTION_TEMPLATE\n",
    "\n",
    "class EvaluateSolutionPrompt(BasePrompt):\n",
    "    system_prompt: str = SYSTEM_PROMPT\n",
    "    user_prompt: str = EVALUATE_SOLUTION_TEMPLATE\n",
    "\n",
    "class ImproveSolutionPrompt(BasePrompt):\n",
    "    system_prompt: str = SYSTEM_PROMPT\n",
    "    user_prompt: str = IMPROVE_SOLUTION_TEMPLATE\n",
    "\n",
    "class DecisionPrompt(BasePrompt):\n",
    "    system_prompt: str = DECISION_SYSTEM_PROMPT\n",
    "    user_prompt: str = DECISION_PROMPT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our Nodes\n",
    "Use plain python functions like we've done in the previous labs to create our node logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, Dict, Any, List, TypedDict\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from utils.base_prompt import BasePrompt\n",
    "\n",
    "# Define the WorkflowState using TypedDict.\n",
    "class WorkflowState(TypedDict):\n",
    "    question: str\n",
    "    answer: str = None\n",
    "    context: str = None\n",
    "    feedback: str = None\n",
    "    iteration: int = 0  \n",
    "    final_answer: str = None\n",
    "\n",
    "def generate_answer(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"Generates an initial answer using RAG\"\"\"\n",
    "    \n",
    "    # Make sure we have a question in the state\n",
    "    question: str = state.get('question')\n",
    "\n",
    "    # Get context from VectorDB\n",
    "    context: str = do_retrieve(question)\n",
    "\n",
    "    # Build inputs\n",
    "    inputs: Dict[str, any] = {\n",
    "        'question': question,\n",
    "        'context': context\n",
    "    }\n",
    "\n",
    "    rag_prompt: BasePrompt = GenerateSolutionPrompt(inputs=inputs)\n",
    "    \n",
    "    # Use the do_rag helper function with the GenerateSolutionPrompt\n",
    "    answer: str = call_bedrock(rag_prompt)\n",
    "\n",
    "    # Update the state dictionary.\n",
    "    state[\"answer\"] = answer\n",
    "    state[\"context\"] = context\n",
    "    state['iteration'] = state['iteration'] + 1\n",
    "    \n",
    "    # Return updated state with the answer and initialize iteration counter\n",
    "    return state\n",
    "\n",
    "def evaluate_answer(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"Evaluates the quality of the answer\"\"\"\n",
    "\n",
    "    print(f\"Evaluating answer to question: {state['question']}\")\n",
    "    inputs: Dict[str, Any] = {\n",
    "        \"question\": state[\"question\"],\n",
    "        \"answer\": state[\"answer\"],\n",
    "        \"context\": state[\"context\"]\n",
    "    }\n",
    "\n",
    "    evaluate_prompt: BasePrompt = EvaluateSolutionPrompt(inputs=inputs)\n",
    "    \n",
    "    # Call Bedrock to get evaluation feedback\n",
    "    feedback: str = call_bedrock(evaluate_prompt)\n",
    "    state['feedback'] = feedback\n",
    "    \n",
    "    # Return updated state with feedback\n",
    "    return state\n",
    "\n",
    "def should_improve(state: WorkflowState) -> str:\n",
    "    \"\"\"Decides whether to improve or finalize based on evaluation\"\"\"\n",
    "    \n",
    "    # Limit to maximum 2 improvement iterations\n",
    "    if state[\"iteration\"] >= 2:\n",
    "        return \"DONE\"\n",
    "        \n",
    "    # Create a simple prompt to decide if improvement is needed\n",
    "    inputs: Dict[str, Any] = {\n",
    "        'feedback': state['feedback']\n",
    "    }\n",
    "    decision_prompt: BasePrompt = DecisionPrompt(inputs=inputs)\n",
    "    \n",
    "    # Get the decision\n",
    "    decision: str = call_bedrock(decision_prompt).strip()\n",
    "    \n",
    "    # Default to DONE if decision isn't clearly IMPROVE\n",
    "    return \"IMPROVE\" if \"IMPROVE\" in decision else \"DONE\"\n",
    "\n",
    "def improve_answer(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"Improves the answer based on feedback using RAG\"\"\"\n",
    "\n",
    "    # Just do the retrieve portion of RAG.\n",
    "    context: str = do_retrieve(state[\"question\"])\n",
    "\n",
    "    # Build inputs.\n",
    "    inputs={\n",
    "        \"question\": state[\"question\"],\n",
    "        \"answer\": state[\"answer\"],\n",
    "        \"feedback\": state[\"feedback\"],\n",
    "        \"context\": context\n",
    "    }\n",
    "    \n",
    "    # Create the improvement prompt\n",
    "    improve_prompt = ImproveSolutionPrompt(inputs=inputs)\n",
    "    \n",
    "    # Call Bedrock to get improved answer\n",
    "    improved_answer: str = call_bedrock(improve_prompt)\n",
    "\n",
    "    state[\"answer\"] = improved_answer\n",
    "    state[\"iteration\"] = state[\"iteration\"] + 1\n",
    "\n",
    "    return state\n",
    "\n",
    "def finalize_answer(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"Finalizes the answer\"\"\"\n",
    "    # The final answer is the current answer\n",
    "    state[\"final_answer\"] = state[\"answer\"]\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Our Graph\n",
    "Build and compile the evaluator optimizer graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_evaluator_optimizer_workflow():\n",
    "    \"\"\"Builds the evaluator-optimizer workflow\"\"\"\n",
    "    workflow = StateGraph(WorkflowState)\n",
    "\n",
    "    # Add nodes to the graph\n",
    "    workflow.add_node(\"generate\", generate_answer)\n",
    "    workflow.add_node(\"evaluate\", evaluate_answer)\n",
    "    workflow.add_node(\"improve\", improve_answer)\n",
    "    workflow.add_node(\"finalize\", finalize_answer)\n",
    "    \n",
    "    # Connect the workflow\n",
    "    workflow.add_edge(START, \"generate\")\n",
    "    workflow.add_edge(\"generate\", \"evaluate\")\n",
    "\n",
    "    # Build decision map. \n",
    "    decision_map: Dict[str, str] = {\n",
    "        \"IMPROVE\": \"improve\",\n",
    "        \"DONE\": \"finalize\"\n",
    "    }\n",
    "\n",
    "    # Add conditional edges.\n",
    "    workflow.add_conditional_edges(\"evaluate\", should_improve, decision_map)\n",
    "\n",
    "    # Create the improvement loop\n",
    "    workflow.add_edge(\"improve\", \"evaluate\")\n",
    "\n",
    "    # Finalize the workflow\n",
    "    workflow.add_edge(\"finalize\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Create our workflow\n",
    "evaluator_optimizer: StateGraph = build_evaluator_optimizer_workflow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualize the graph to get a sense of what we're about to run\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "display(\n",
    "    Image(\n",
    "        evaluator_optimizer.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_state(question: str) -> WorkflowState:\n",
    "    \"\"\"Initialize the workflow state with a question.\"\"\"\n",
    "    return WorkflowState(\n",
    "        question=question,\n",
    "        answer=\"\",\n",
    "        context=\"\",\n",
    "        feedback=\"\",\n",
    "        iteration=0,\n",
    "        final_answer=None\n",
    "    )\n",
    "\n",
    "# Initialize with a test question - make sure to use \"question\" as the key\n",
    "question: str = \"OpenSearch cluster showing red health status and not responding to queries\"\n",
    "initial_state: WorkflowState = init_state(question)\n",
    "\n",
    "# Run the evaluator-optimizer workflow\n",
    "result: WorkflowState = evaluator_optimizer.invoke(initial_state)\n",
    "\n",
    "print(\"üîç Optimized Solution\\n\")\n",
    "print(result[\"final_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benefits of the Evaluator-Optimizer Pattern\n",
    "\n",
    "Our iterative improvement approach provides several advantages:\n",
    "\n",
    "‚úÖ Systematic quality improvement through feedback loops\n",
    "\n",
    "‚úÖ Clear evaluation criteria for consistency\n",
    "\n",
    "‚úÖ Automatic refinement without human intervention\n",
    "\n",
    "‚úÖ Better answers through multiple improvement iterations\n",
    "\n",
    "Remember: While this pattern is powerful, use it judiciously where the extra compute cost is justified by measurable quality improvements! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
