{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Platform: Agent Evaluation\n",
    "\n",
    "This lab introduces the concept of Agent Evaluation. Effective evaluation frameworks are essential for measuring, comparing and improving agent performance across different implementations and configurations.\n",
    "\n",
    "There are several approaches to evaluating agentic systems, ranging from simple to complex. In this lab we'll focus on two evaluation metrics\n",
    "* Assertion based evaluations\n",
    "* Step-based procedural analysis\n",
    "\n",
    "The most sophisticated evaluation systems incorporate human feedback loops and nuanced assessments of agent behavior. While powerful, these systems often require significant infrastructure and human resources to implement effectively.\n",
    "\n",
    "To get started, we'll explore a couple approaches to an evaluation framework focused on two key metrics:\n",
    "1. Task success rate - measuring whether agents complete their assigned tasks correctly\n",
    "2. Steps to completion - analyzing the efficiency of agents by tracking the number of steps required to achieve success\n",
    "\n",
    "Assertions are essentially test cases. Sometimes we're looking for exact answers (What's the capital of France? -> Paris). Sometimes the evaluation criteria is qualitative data (did the agent provide a recipe that was gluten free?)\n",
    "\n",
    "These metrics provide a straightforward but powerful foundation for comparing agent performance across different models, prompting strategies, and tool configurations.\n",
    "\n",
    "First lets modify a couple agents we built in the previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel\n",
    "from tavily import TavilyClient\n",
    "from typing import Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from agentic_platform.core.models.memory_models import ToolResult\n",
    "from mcp import ListToolsResult\n",
    "from mcp.server import FastMCP\n",
    "from typing import List, Any\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# First we wrap our own MCP server in a MCPServerStdio object\n",
    "from pydantic_ai.mcp import MCPServerStdio as PyAIMCPServerStdio\n",
    "\n",
    "# Load our API key from the environment variable\n",
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')\n",
    "\n",
    "# Now lets create our research tools\n",
    "class WebSearch(BaseModel):\n",
    "    query: str\n",
    "\n",
    "server = FastMCP()\n",
    "\n",
    "def search_web(query: WebSearch) -> List[Dict[str, Any]]:\n",
    "    '''Search the web to get back a list of results and content.'''\n",
    "    client: TavilyClient = TavilyClient(os.getenv(\"TAVILY_KEY\"))\n",
    "    response: Dict[str, any] = client.search(query=query.query)\n",
    "    return response['results']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next lets create our researcher agent. \n",
    "from pydantic_ai import Agent as PyAIAgent\n",
    "from agentic_platform.core.models.prompt_models import BasePrompt\n",
    "from pydantic import BaseModel\n",
    "\n",
    "RESEARCHER_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful research agent with web search capabilities. Your job is to:\n",
    "\n",
    "1. Search for accurate, up-to-date information about any topic\n",
    "2. Provide clear and conscise answers to the users question. Provide a source annotation at the end of your response that maps to the source links below.\n",
    "3. Cite your sources with numbered links at the end of your response.\n",
    "\n",
    "Always be factual and objective in your research. Be clear and concise in your responses.\n",
    "Only answer the immediate question, you do not need to provide any additional context or commentary.\n",
    "If someone asks who the CEO of a company is, you should just say their name for example.\n",
    "\"\"\"\n",
    "\n",
    "# Define the expected output structure\n",
    "class ResearchResponse(BaseModel):\n",
    "    content: str\n",
    "    sources: list[str]\n",
    "\n",
    "# Create the agent with the simplified prompt\n",
    "researcher_agent = PyAIAgent(\n",
    "    'bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "    system_prompt=RESEARCHER_SYSTEM_PROMPT,\n",
    "    result_type=ResearchResponse,\n",
    ")\n",
    "\n",
    "researcher_agent.tool_plain(search_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "test_case = \"Who is the CEO of Amazon?\"\n",
    "\n",
    "result = researcher_agent.run_sync(test_case)\n",
    "\n",
    "result.output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the conversation history to see where the results came from. \n",
    "result.all_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Assertions.\n",
    "Now that we have a researcher agent using Tavily websearch, lets evaluate how well it does. There are many frameworks that do evaluations for you including LangChain, Ragas, and Pydantic Evals. However, it's valuable to understand whats going on under the hood by at least seeing it written out from scratch. \n",
    "\n",
    "Below is a sample implementation of an assertion based evaluator. In practice, you'd want to parallelize these for speed like we did in the module 1 evaluation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import boto3\n",
    "from typing import List, Callable, Awaitable\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class TestCase(BaseModel):\n",
    "    name: str\n",
    "    query: str\n",
    "    assertions: List[str]\n",
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    content: str\n",
    "    sources: List[str]\n",
    "    success: bool\n",
    "    error: str = None\n",
    "\n",
    "class AssertionResult(BaseModel):\n",
    "    assertion: str\n",
    "    passed: bool\n",
    "\n",
    "class TestResult(BaseModel):\n",
    "    name: str\n",
    "    query: str\n",
    "    assertions_results: List[AssertionResult]\n",
    "    agent_failed: bool = False\n",
    "    agent_error: str = None\n",
    "    \n",
    "    @property\n",
    "    def total_assertions(self) -> int:\n",
    "        return len(self.assertions_results)\n",
    "    \n",
    "    @property\n",
    "    def passed_assertions(self) -> int:\n",
    "        return sum(1 for r in self.assertions_results if r.passed)\n",
    "    \n",
    "    @property\n",
    "    def success_rate(self) -> float:\n",
    "        if self.agent_failed or self.total_assertions == 0:\n",
    "            return 0.0\n",
    "        return self.passed_assertions / self.total_assertions\n",
    "\n",
    "class AssertionEvaluator:\n",
    "    def __init__(self, model_id: str = 'us.anthropic.claude-3-5-haiku-20241022-v1:0', region: str = 'us-east-1'):\n",
    "        self.bedrock = boto3.client('bedrock-runtime', region_name=region)\n",
    "        self.model_id = model_id\n",
    "    \n",
    "    def _check_assertion(self, query: str, response: AgentResponse, assertion: str) -> bool:\n",
    "        prompt = f\"\"\"Check if this assertion is TRUE or FALSE based on the research response.\n",
    "        \n",
    "        QUERY: {query}\n",
    "        RESPONSE: {response.content}\n",
    "        SOURCES: {response.sources}\n",
    "        \n",
    "        ASSERTION TO CHECK: {assertion}\n",
    "        \n",
    "        Return only \"TRUE\" or \"FALSE\" (no explanation needed).\"\"\"\n",
    "        \n",
    "        try:\n",
    "            resp = self.bedrock.converse(\n",
    "                modelId=self.model_id,\n",
    "                messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "                inferenceConfig={\"maxTokens\": 50, \"temperature\": 0}\n",
    "            )\n",
    "            content = resp['output']['message']['content'][0]['text'].strip().upper()\n",
    "            return \"TRUE\" in content\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking assertion: {e}\")\n",
    "            return False\n",
    "    \n",
    "    async def evaluate_test_case(self, test_case: TestCase, agent_function: Callable[[str], Awaitable[AgentResponse]]) -> TestResult:\n",
    "        print(f\"ðŸ§ª Running: {test_case.name}\")\n",
    "        \n",
    "        response = await agent_function(test_case.query)\n",
    "        \n",
    "        if not response.success:\n",
    "            print(f\"âŒ Agent failed: {response.error}\")\n",
    "            return TestResult(\n",
    "                name=test_case.name,\n",
    "                query=test_case.query,\n",
    "                assertions_results=[],\n",
    "                agent_failed=True,\n",
    "                agent_error=response.error\n",
    "            )\n",
    "        \n",
    "        print(f\"ðŸ’¬ Response: {response.content[:100]}...\")\n",
    "        \n",
    "        assertion_results = []\n",
    "        for i, assertion in enumerate(test_case.assertions, 1):\n",
    "            print(f\"  Checking assertion {i}...\")\n",
    "            passed = self._check_assertion(test_case.query, response, assertion)\n",
    "            assertion_results.append(AssertionResult(assertion=assertion, passed=passed))\n",
    "        \n",
    "        return TestResult(\n",
    "            name=test_case.name,\n",
    "            query=test_case.query,\n",
    "            assertions_results=assertion_results\n",
    "        )\n",
    "    \n",
    "    async def evaluate_test_cases(self, test_cases: List[TestCase], agent_function: Callable[[str], Awaitable[AgentResponse]]) -> List[TestResult]:\n",
    "        results = []\n",
    "        for test_case in test_cases:\n",
    "            result = await self.evaluate_test_case(test_case, agent_function)\n",
    "            results.append(result)\n",
    "            print()\n",
    "        return results\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_results(results: List[TestResult]):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸ“Š EVALUATION RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for result in results:\n",
    "            print(f\"\\n{result.name}: {result.passed_assertions}/{result.total_assertions} passed\")\n",
    "            \n",
    "            if result.agent_failed:\n",
    "                print(f\"  âŒ Agent execution failed: {result.agent_error}\")\n",
    "                continue\n",
    "                \n",
    "            for j, assertion_result in enumerate(result.assertions_results, 1):\n",
    "                symbol = \"âœ…\" if assertion_result.passed else \"âŒ\"\n",
    "                status = \"PASSED\" if assertion_result.passed else \"FAILED\"\n",
    "                print(f\"  {symbol} Assertion {j} ({status}): {assertion_result.assertion}\")\n",
    "        \n",
    "        successful_results = [r for r in results if not r.agent_failed]\n",
    "        total_tests = len(results)\n",
    "        successful_tests = len(successful_results)\n",
    "        \n",
    "        if successful_tests > 0:\n",
    "            total_assertions = sum(r.total_assertions for r in successful_results)\n",
    "            passed_assertions = sum(r.passed_assertions for r in successful_results)\n",
    "            \n",
    "            print(f\"\\nðŸ“ˆ SUMMARY:\")\n",
    "            print(f\"  Tests: {successful_tests}/{total_tests} successful\")\n",
    "            print(f\"  Assertions: {passed_assertions}/{total_assertions} passed\")\n",
    "            print(f\"  Success Rate: {(passed_assertions/total_assertions)*100:.1f}%\")\n",
    "            \n",
    "            # Show failed assertions\n",
    "            failed_assertions = []\n",
    "            for result in successful_results:\n",
    "                for i, assertion_result in enumerate(result.assertions_results, 1):\n",
    "                    if not assertion_result.passed:\n",
    "                        failed_assertions.append(f\"{result.name} - Assertion {i}: {assertion_result.assertion}\")\n",
    "            \n",
    "            if failed_assertions:\n",
    "                print(f\"\\nâŒ FAILED ASSERTIONS:\")\n",
    "                for failed in failed_assertions:\n",
    "                    print(f\"  â€¢ {failed}\")\n",
    "        else:\n",
    "            print(f\"\\nâŒ All {total_tests} tests failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Our Assertions\n",
    "Next we'll create a function to evaluate against and pass in a test set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the researcher agent\n",
    "async def researcher_agent_function(query: str) -> AgentResponse:\n",
    "    try:\n",
    "        async with researcher_agent.run_mcp_servers():\n",
    "            result = researcher_agent.run_sync(query)\n",
    "        return AgentResponse(\n",
    "            content=result.output.content,\n",
    "            sources=result.output.sources,\n",
    "            success=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return AgentResponse(\n",
    "            content=\"\",\n",
    "            sources=[],\n",
    "            success=False,\n",
    "            error=str(e)\n",
    "        )\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    TestCase(\n",
    "        name='Amazon CEO Test',\n",
    "        query='Who is the current CEO of Amazon?',\n",
    "        assertions=[\n",
    "            'The response correctly identifies Andy Jassy as the current CEO of Amazon',\n",
    "            'The response mentions when Andy Jassy became CEO (2021)',\n",
    "            'The response includes at least one credible source citation',\n",
    "            'The response is concise and directly answers the question'\n",
    "        ]\n",
    "    ),\n",
    "    TestCase(\n",
    "        name='Paris Population Test',\n",
    "        query='What is the population of the capital of France?',\n",
    "        assertions=[\n",
    "            'The response correctly identifies Paris as the capital of France',\n",
    "            'The response provides a specific population figure for Paris',\n",
    "            'The response distinguishes between city proper and metropolitan area population',\n",
    "            'The response cites at least two reliable sources',\n",
    "            'The response indicates when the population data was collected/estimated'\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Usage example\n",
    "async def run_evaluation():\n",
    "    print(\"ðŸš€ Starting Assertion-Based Evaluation\\n\")\n",
    "    evaluator = AssertionEvaluator()\n",
    "    results = await evaluator.evaluate_test_cases(test_cases, researcher_agent_function)\n",
    "    evaluator.print_results(results)\n",
    "\n",
    "# Execute\n",
    "await run_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we got our results back. Evals are finicky. They almost never pass 100%, but they do fail. Running them multiple times can often result in different answers by the nature of LLMs being non-determinstic. What consistitutes as passing is more on the owner of the change. One metric might go up while another goes down. \n",
    "\n",
    "Lastly, writing all your cases in code is not ideal. Let's export this dataset to a json file so it's more reusable across different platforms and approaches to evaluation. We can write utility functions to import and export them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Utility functions for test case management\n",
    "def export_test_cases(test_cases: List[TestCase], file_path: str):\n",
    "    \"\"\"Export test cases to JSON file\"\"\"\n",
    "    from pathlib import Path\n",
    "    data = [test_case.model_dump() for test_case in test_cases]\n",
    "    Path(file_path).write_text(json.dumps(data, indent=2))\n",
    "    print(f\"âœ… Exported {len(test_cases)} test cases to {file_path}\")\n",
    "\n",
    "def import_test_cases(file_path: str) -> List[TestCase]:\n",
    "    \"\"\"Import test cases from JSON file\"\"\"\n",
    "    from pathlib import Path\n",
    "    data = json.loads(Path(file_path).read_text())\n",
    "    test_cases = [TestCase(**item) for item in data]\n",
    "    print(f\"âœ… Imported {len(test_cases)} test cases from {file_path}\")\n",
    "    return test_cases\n",
    "\n",
    "# Export test cases\n",
    "test_cases_path = Path('data/test_cases.json')\n",
    "export_test_cases(test_cases, str(test_cases_path))\n",
    "\n",
    "# Import test cases  \n",
    "loaded_test_cases = import_test_cases(str(test_cases_path))\n",
    "\n",
    "# View the saved file\n",
    "print(test_cases_path.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Use Evaluation\n",
    "Next lets go a bit deeper and evaluate tool use. Every evaluation framework has a different method of doing this. Ragas (popular) measures tool adherence, tool call accuracy, etc.. We'll opt for a slightly different approach. And because none of the frameworks support it, we'll write it ourselves. \n",
    "\n",
    "Agents can take undeterministic paths. The important thing is that they get to the right answer in a reasonable amount of steps. In cases where we do know the steps the agent would need to take, we can create logical groupings of tool invocations. Below is an implementation of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.core.models.memory_models import Message, SessionContext\n",
    "from pydantic_ai.agent import AgentRunResult\n",
    "\n",
    "test_runs: Dict[str, Any] = {}\n",
    "\n",
    "# Define the input type for the researcher agent\n",
    "class ResearchQuery(BaseModel):\n",
    "    user_query: str\n",
    "\n",
    "# Updated to use assertion evaluator\n",
    "async def perform_research_2(query: str) -> AgentResponse:\n",
    "    try:\n",
    "        async with researcher_agent.run_mcp_servers():\n",
    "            result: AgentRunResult[ResearchResponse] = researcher_agent.run_sync(query)\n",
    "        \n",
    "        test_runs[query] = result.all_messages()\n",
    "        \n",
    "        return AgentResponse(\n",
    "            content=result.output.content,\n",
    "            sources=result.output.sources,\n",
    "            success=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return AgentResponse(\n",
    "            content=\"\",\n",
    "            sources=[],\n",
    "            success=False,\n",
    "            error=str(e)\n",
    "        )\n",
    "\n",
    "# Run the assertion evaluation\n",
    "print(\"ðŸš€ Starting Assertion-Based Evaluation\\n\")\n",
    "evaluator = AssertionEvaluator()\n",
    "results = await evaluator.evaluate_test_cases(test_cases, perform_research_2)\n",
    "evaluator.print_results(results)\n",
    "\n",
    "print('-' * 100)\n",
    "\n",
    "for k, v in test_runs.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets convert our test runs into our format so we can run our own evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets convert the message to our types using our converters we've already build. \n",
    "from typing import List\n",
    "from agentic_platform.core.converter.pydanticai_converters import PydanticAIMessageConverter\n",
    "\n",
    "# Convert the messages to our types.\n",
    "test_runs_formatted: Dict[str, List[Message]] = {}\n",
    "for k, v in test_runs.items():\n",
    "    print(v)\n",
    "    test_runs_formatted[k] = PydanticAIMessageConverter.convert_messages(v)\n",
    "\n",
    "for k, v in test_runs_formatted.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print('-' * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, lets write our evaluation harness. To do this we need our test cases. Steps to complete is performed by creating logical groupings of steps the agent takes to complete a task. Because they're autonomous, they might take different paths to get to the same answer. This is why logical groupings are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Callable\n",
    "from pydantic import Field\n",
    "import json\n",
    "from typing import Tuple\n",
    "from pydantic_core import to_jsonable_python\n",
    "from agentic_platform.core.models.llm_models import LLMRequest, LLMResponse\n",
    "from agentic_platform.core.models.prompt_models import BasePrompt\n",
    "from agentic_platform.core.models.tool_models import ToolSpec\n",
    "\n",
    "from agentic_platform.core.converter.llm_request_converters import ConverseRequestConverter\n",
    "from agentic_platform.core.converter.llm_response_converters import ConverseResponseConverter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import boto3\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert evaluator of agentic systems. You are given a list of steps the agent took to complete a task and a list of expected steps. \n",
    "You need to evaluate the agent's performance based on the expected steps.\n",
    "\n",
    "You should take the steps inputted and create \"logical groupings\" of those steps. Those grouping names should come from the expected steps if similar. \n",
    "If the agent took a different path, you should create a new grouping name so we can evaluate the agent's performance.\n",
    "\n",
    "When creating logical groupings, group things together. Instead of saying search the web, gather results. Group them into one step even if the message history shows them as separate steps.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Here are the steps the agent took to complete the task:\n",
    "{steps}\n",
    "\n",
    "Here are the expected steps:\n",
    "{expected_steps}\n",
    "\n",
    "Here is the task success criteria:\n",
    "{success_criteria}\n",
    "\n",
    "Before answering, think about the steps and how the agent took them to complete the task.\n",
    "\"\"\"\n",
    "\n",
    "# Types for the evaluation.\n",
    "\n",
    "class AgentEvalPrompt(BasePrompt):\n",
    "    system_prompt: str = SYSTEM_PROMPT\n",
    "    user_prompt: str = USER_PROMPT\n",
    "\n",
    "class AgentEvalResult(BaseModel):\n",
    "    '''Evaluation results for the agent.'''\n",
    "    thoughts: str = Field(\n",
    "        description=\"The evaluators thoughts on the agents performance.\"\n",
    "    )\n",
    "    \n",
    "    steps: List[str] = Field(\n",
    "        description=\"The logical groupings of actions taken by the agent to complete the task, typically representing tool calls or major decision points.\"\n",
    "    )\n",
    "    \n",
    "    task_success: bool = Field(\n",
    "        description=\"Whether the agent successfully completed the task according to the defined success criteria. True indicates success, False indicates failure.\"\n",
    "    )\n",
    "\n",
    "class AgentEvalSample(BaseModel):\n",
    "    user_input: str\n",
    "    expected_steps: List[str]\n",
    "    expected_output: Any\n",
    "    success_criteria: Literal['Got Corect Answer', '< 1 step from gold standard']\n",
    "\n",
    "\n",
    "client = boto3.client('bedrock-runtime')\n",
    "\n",
    "# Helper function to call the LLM.\n",
    "def call_bedrock(request: LLMRequest) -> LLMResponse:\n",
    "    kwargs: Dict[str, Any] = ConverseRequestConverter.convert_llm_request(request)\n",
    "    converse_response: Dict[str, Any] = client.converse(**kwargs)\n",
    "    return ConverseResponseConverter.to_llm_response(converse_response)\n",
    "\n",
    "\n",
    "def eval_function(sample: AgentEvalSample, context: List[Message]) -> AgentEvalResult:\n",
    "    # Create the input for the prompt.\n",
    "    inputs={\n",
    "        'steps': json.dumps(to_jsonable_python(context)),\n",
    "        'expected_steps': '\\n'.join(sample.expected_steps),\n",
    "        'success_criteria': sample.success_criteria\n",
    "    }\n",
    "\n",
    "    # Format the prompt.\n",
    "    prompt: BasePrompt = AgentEvalPrompt(inputs=inputs)\n",
    "\n",
    "    # Create a tool spec for the structured output.\n",
    "    tool_spec: ToolSpec = ToolSpec(\n",
    "        name='AgentEvalResult',\n",
    "        description='Structured output for an agents performance.',\n",
    "        model=AgentEvalResult,\n",
    "    )\n",
    "\n",
    "    # Create the LLM request and forces structured output through a tool call.\n",
    "    llm_request: LLMRequest = LLMRequest(\n",
    "        system_prompt=prompt.system_prompt,\n",
    "        model_id='us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "        messages=[Message(role='user', text=prompt.user_prompt)],\n",
    "        hyperparams={\"temperature\": 0.2},\n",
    "        tools=[tool_spec],\n",
    "        force_tool=tool_spec.name\n",
    "    )\n",
    "\n",
    "    # Call the LLM and get results.\n",
    "    llm_response: LLMResponse = call_bedrock(llm_request)\n",
    "    return AgentEvalResult(**llm_response.tool_calls[0].arguments)\n",
    "\n",
    "def run_function(sample: AgentEvalSample) -> List[Message]:\n",
    "    result: AgentRunResult[ResearchResponse] = researcher_agent.run_sync(sample.user_input)\n",
    "    messages: List[Message] = result.all_messages()\n",
    "    return PydanticAIMessageConverter.convert_messages(messages)\n",
    "\n",
    "class AgentEvalHarness:\n",
    "\n",
    "    def __init__(self, \n",
    "                 samples: List[AgentEvalSample], \n",
    "                 eval_function: Callable[[AgentEvalSample, List[Message]], AgentEvalResult],\n",
    "                 run_function: Callable[[AgentEvalSample], List[Message]]):\n",
    "        \n",
    "        self.samples = samples\n",
    "        self.eval_function = eval_function\n",
    "        self.run_function = run_function\n",
    "\n",
    "    def evaluate_sample(self, sample: AgentEvalSample) -> Tuple[AgentEvalSample, AgentEvalResult]:\n",
    "        context: List[Message] = self.run_function(sample)\n",
    "        result: AgentEvalResult = self.eval_function(sample, context)\n",
    "        return sample, result\n",
    "    \n",
    "    \n",
    "    def evaluate_threaded(self, num_workers: int = 2) -> List[AgentEvalSample]:\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            results: List[Tuple[AgentEvalSample, AgentEvalResult]] = list(executor.map(self.evaluate_sample, self.samples))\n",
    "        return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    AgentEvalSample(\n",
    "        user_input='Who is the current CEO of Amazon?',\n",
    "        expected_steps=['Search the web for the current CEO of Amazon', \"Verify the information is correct\"],\n",
    "        expected_output='Andy Jassy',\n",
    "        success_criteria='Got Corect Answer'\n",
    "    ),\n",
    "    AgentEvalSample(\n",
    "        user_input='What is the population of the capital of France?',\n",
    "        expected_steps=[\n",
    "            'Search the web for the capital of France',\n",
    "            'Search the web for the population of Paris',\n",
    "            'Format results into a response'\n",
    "        ],\n",
    "        expected_output='Roughly 2.1 million',\n",
    "        success_criteria='Got Corect Answer'\n",
    "    )\n",
    "]\n",
    "\n",
    "harness = AgentEvalHarness(samples, eval_function, run_function)\n",
    "evaluation_results = harness.evaluate_threaded()\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, but going through each result individually would be painful. We can create a summary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_summary(eval_pairs: List[Tuple[AgentEvalSample, AgentEvalResult]]) -> str:\n",
    "    total_samples = len(eval_pairs)\n",
    "    success_count = 0\n",
    "    total_step_delta = 0\n",
    "    \n",
    "    for sample, result in eval_pairs:\n",
    "        # Calculate step delta\n",
    "        expected_step_count = len(sample.expected_steps)\n",
    "        actual_step_count = len(result.steps)\n",
    "        step_delta = actual_step_count - expected_step_count\n",
    "        \n",
    "        # Update totals\n",
    "        if result.task_success:\n",
    "            success_count += 1\n",
    "        total_step_delta += step_delta\n",
    "    \n",
    "    # Calculate averages\n",
    "    success_rate = success_count / total_samples * 100\n",
    "    avg_step_delta = total_step_delta / total_samples\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = f\"\"\"\n",
    "    AGENT EVALUATION SUMMARY\n",
    "    ========================\n",
    "    Total samples evaluated: {total_samples}\n",
    "    Success rate: {success_rate:.1f}%\n",
    "    Average step delta: {avg_step_delta:.2f}\n",
    "\n",
    "    Step delta interpretation:\n",
    "    - Negative: Agent used fewer steps than expected (more efficient)\n",
    "    - Zero: Agent used exactly the expected number of steps\n",
    "    - Positive: Agent used more steps than expected (less efficient)\n",
    "    \"\"\"\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_evaluation_summary(eval_pairs=evaluation_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, lets convert the results to a pandas dataframe so we can dive into the results a bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "def create_evaluation_dataframe(eval_pairs: List[Tuple]):\n",
    "    \"\"\"Convert evaluation pairs to a structured pandas DataFrame.\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    for sample, result in eval_pairs:\n",
    "        # Calculate step delta\n",
    "        expected_step_count = len(sample.expected_steps)\n",
    "        actual_step_count = len(result.steps)\n",
    "        step_delta = actual_step_count - expected_step_count\n",
    "        \n",
    "        # Create row\n",
    "        row = {\n",
    "            'user_input': sample.user_input,\n",
    "            'expected_steps': sample.expected_steps,\n",
    "            'expected_output': sample.expected_output,\n",
    "            'success_criteria': sample.success_criteria,\n",
    "            'actual_steps': result.steps,\n",
    "            'step_count': actual_step_count,\n",
    "            'expected_step_count': expected_step_count,\n",
    "            'step_delta': step_delta,\n",
    "            'task_success': result.task_success,\n",
    "            'thoughts': result.thoughts\n",
    "        }\n",
    "        data.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Calculate summary statistics and add to dataframe attributes\n",
    "    df.attrs['total_samples'] = len(eval_pairs)\n",
    "    df.attrs['success_rate'] = df['task_success'].mean() * 100\n",
    "    df.attrs['avg_step_delta'] = df['step_delta'].mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "df = create_evaluation_dataframe(evaluation_results)\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Evaluation results for {df.attrs['total_samples']} samples:\")\n",
    "print(f\"Success rate: {df.attrs['success_rate']:.1f}%\")\n",
    "print(f\"Average step delta: {df.attrs['avg_step_delta']:.2f}\")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nDetailed results:\")\n",
    "print(df[['user_input', 'step_delta', 'task_success']])\n",
    "\n",
    "# Output summary statistics\n",
    "print(\"\\nSummary by success status:\")\n",
    "print(df.groupby('task_success')['step_delta'].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this lab we went through the basics of agent evaluation. This is very much an open research area. For starting out, we suggest using the assertion based approach that Pydantic provides. The most important thing in evaluation is to identify what metrics to select and what make sense for your usecase. If a framework provides it, great. If not, you often times have to build your own test harness. \n",
    "\n",
    "In future labs, we'll discuss how these fit into a CI/CD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
